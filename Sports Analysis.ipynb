{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7996508d-10c2-42ca-b1a3-5ca567778319",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+\n|user_id|kit_id|login_date|sessions_count|\n+-------+------+----------+--------------+\n|      1|     2|2016-03-01|             5|\n|      1|     2|2016-03-02|             6|\n|      2|     3|2017-06-25|             1|\n|      3|     1|2016-03-02|             0|\n|      3|     4|2018-07-03|             5|\n+-------+------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField,StructType,IntegerType,StringType\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"kit_id\", IntegerType(), True),\n",
    "    StructField(\"login_date\", StringType(), True),\n",
    "    StructField(\"sessions_count\", IntegerType(), True)\n",
    "])\n",
    "data = [\n",
    "    (1, 2, \"2016-03-01\", 5),\n",
    "    (1, 2, \"2016-03-02\", 6),\n",
    "    (2, 3, \"2017-06-25\", 1),\n",
    "    (3, 1, \"2016-03-02\", 0),\n",
    "    (3, 4, \"2018-07-03\", 5)\n",
    "]\n",
    "\n",
    "sport_df = spark.createDataFrame(data, schema=schema)\n",
    "sport_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e26166-9ac4-4e59-8e45-5703eba3667c",
     "showTitle": true,
     "title": "First Approach:Group By and Min"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|user_id|min(login_date)|\n+-------+---------------+\n|      1|     2016-03-01|\n|      2|     2017-06-25|\n|      3|     2016-03-02|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df = sport_df.groupBy(\"user_id\").agg(min(\"login_date\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8667a127-d341-435d-8e51-ae13902f8685",
     "showTitle": true,
     "title": "second approach:windows functions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+---+\n|user_id|kit_id|login_date|sessions_count|rnk|\n+-------+------+----------+--------------+---+\n|      1|     2|2016-03-01|             5|  1|\n|      2|     3|2017-06-25|             1|  1|\n|      3|     1|2016-03-02|             0|  1|\n+-------+------+----------+--------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank,col\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
    "df = sport_df.withColumn(\"rnk\",dense_rank().over(window_spec))\n",
    "df.filter(col(\"rnk\") == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ca8092-d919-4d03-a519-f14d69ee9e0a",
     "showTitle": true,
     "title": "first kit used by player"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+---+\n|user_id|kit_id|login_date|sessions_count|rnk|\n+-------+------+----------+--------------+---+\n|      1|     2|2016-03-01|             5|  1|\n|      2|     3|2017-06-25|             1|  1|\n|      3|     1|2016-03-02|             0|  1|\n+-------+------+----------+--------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank,col\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
    "df = sport_df.withColumn(\"rnk\",dense_rank().over(window_spec))\n",
    "df.filter(col(\"rnk\") == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a341363b-746d-4c5d-9aff-96d5b33ef3b3",
     "showTitle": true,
     "title": "total no of games played by the player until that day"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+\n|user_id|kit_id|login_date|sessions_count|\n+-------+------+----------+--------------+\n|      1|     2|2016-03-01|             5|\n|      1|     2|2016-03-02|             6|\n|      2|     3|2017-06-25|             1|\n|      3|     1|2016-03-02|             0|\n|      3|     4|2018-07-03|             5|\n+-------+------+----------+--------------+\n\n+-------+------+----------+--------------+---------+-------------------+\n|user_id|kit_id|login_date|sessions_count|lag_count|games_played_so_far|\n+-------+------+----------+--------------+---------+-------------------+\n|      1|     2|2016-03-01|             5|        0|                  5|\n|      1|     2|2016-03-02|             6|        5|                 11|\n|      2|     3|2017-06-25|             1|        0|                  1|\n|      3|     1|2016-03-02|             0|        0|                  0|\n|      3|     4|2018-07-03|             5|        0|                  5|\n+-------+------+----------+--------------+---------+-------------------+\n\n+-------+------+----------+--------------+-------------------+\n|user_id|kit_id|login_date|sessions_count|games_played_so_far|\n+-------+------+----------+--------------+-------------------+\n|      1|     2|2016-03-01|             5|                  5|\n|      1|     2|2016-03-02|             6|                 11|\n|      2|     3|2017-06-25|             1|                  1|\n|      3|     1|2016-03-02|             0|                  0|\n|      3|     4|2018-07-03|             5|                  5|\n+-------+------+----------+--------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sport_df.show()\n",
    "from pyspark.sql.functions import lag,asc,sum\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(asc(\"login_date\"))\n",
    "df = sport_df.withColumn(\"lag_count\" , lag(\"sessions_count\",1,0).over(window_spec))\n",
    "df = df.withColumn(\"games_played_so_far\",col(\"sessions_count\") + col(\"lag_count\"))\n",
    "df.show()\n",
    "#another approach\n",
    "df1 = sport_df.withColumn(\"games_played_so_far\",sum(\"sessions_count\").over(window_spec))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b830df7f-15fc-4e5f-ba1b-999dcca14334",
     "showTitle": true,
     "title": "players that logged infor at least two consective days"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+----------+--------+\n|user_id|kit_id|login_date|sessions_count|  lag_date|diff_col|\n+-------+------+----------+--------------+----------+--------+\n|      1|     2|2016-03-01|             5|2016-03-01|       0|\n|      1|     2|2016-03-02|             6|2016-03-01|       1|\n|      2|     3|2017-06-25|             1|2017-06-25|       0|\n|      3|     1|2016-03-02|             0|2016-03-02|       0|\n|      3|     4|2018-07-03|             5|2016-03-02|     853|\n+-------+------+----------+--------------+----------+--------+\n\n+-------+\n|user_id|\n+-------+\n|      1|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff,min\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
    "df = sport_df.withColumn(\"lag_date\",min(\"login_date\").over(window_spec))\n",
    "df = df.withColumn(\"diff_col\",datediff(\"login_date\",\"lag_date\"))\n",
    "df.show()\n",
    "df.filter(col(\"diff_col\")==1).select(col(\"user_id\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sports Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
